## 1. 원문: DOPE & MOTION, TASK PLANNING
After training the scenes, we spawn the Fetch robot and several objects in a Gazebo environment, which contains the table and bookshelf. It conveys the initial messy scenes to the pre-trained model and the model changes the disordered scenes into ordered one. After successfully generating the goal scenes, we detect 6D object poses from them by using Deep Object Pose Estimation (DOPE). We establish a ROS image publisher which publishes the target images and a camera information from a Fetch robot. Along with the image publisher, DOPE receives the goal images from it, directly estimates the pose of each object and returns the geometry information as a form of ROS topic. Then, the Fetch receives this goal object dimensions and carries on Motion and Task planning from their initial poses to goal poses using MoveIt! and OMPL.


## 2. 들어가야 할 내용

시뮬레이션 환경에서 어질러진 씬을 로봇 카메라로 캡쳐
캡쳐한 알지비 이미지를 앞서 학습시킨 모델의 인풋으로 사용
모델은 정렬된 알지비를 아웃풋으로 리턴
정렬된 아웃풋 이미지에서 3D 오브젝트의 포즈를 estimate하여 시뮬레이션 환경의 goal pose를 결정
로봇은 object를 pick 하여 goal pose에 place함


## 3. 내 커맨트
* implementation details가 많음: 본문에서는 디테일하게보다는 general하게 작성하는것이 중요함. 예를 들어 fetch robot -> mobile robot(?)/ Gazebo environmnet -> simulation environment/ ROS topic 주고받는 이야기도 input/output으로 설명 
* 시스템 오버뷰는 우리가 제작한 시스템에 대한 fact를 설명하는거기 때문에 We~~~ 보다는 passive한 형태로 문장을 구성하는게 좋음!
* System overview는 figure를 통해 설명하는것이 가장 좋음
* Pix2PixHD나 DOPE 같은것 reference 추가 필요
